# AutoImpress: LLM-Based Radiology Impression Generator

![AutoImpress](https://img.shields.io/badge/AutoImpress-Clinical_AI-blue?style=for-the-badge)
![Python](https://img.shields.io/badge/Python-3.8+-green?style=flat-square)
![Transformers](https://img.shields.io/badge/ü§ó_Transformers-4.21.0-yellow?style=flat-square)

*Automated clinical impression generation from structured radiology reports using Large Language Models*

**[üìä Results](#-key-results) ‚Ä¢ [üöÄ Quick Start](#-quick-start) ‚Ä¢ [üìñ Documentation](#-project-structure) ‚Ä¢ [üéØ Architecture](#-methodology)**

---
## üìã Table of Contents

- [üéØ Overview](#-overview)
- [üñºÔ∏è Graphical Abstract](#Ô∏è-graphical-abstract)
  - [üé™ Problem Statement](#-problem-statement)
  - [üèÜ Key Contributions](#-key-contributions)
- [üë• Team](#-team)
- [üì¶ Project Structure](#-project-structure)
  - [üìä Output File Meanings](#-output-file-meanings)
- [üöÄ Quick Start](#-quick-start)
- [üè• Dataset](#-dataset)
  - [üìä Dataset Statistics](#-dataset-statistics)
  - [üîß Preprocessing Steps](#-preprocessing-steps)
- [üß† Methodology](#-methodology)
  - [ü§ñ Models Evaluated](#-models-evaluated)
  - [üîÑ Processing Pipelines](#-processing-pipelines)
  - [üìè Evaluation Metrics](#-evaluation-metrics)
- [üèÜ Key Results](#-key-results)
- [üí° Key Insights](#-key-insights)
- [üî¨ References](#-references)
- [üôè Acknowledgments](#-acknowledgments)

---

## üéØ Overview

AutoImpress is a comprehensive research project that explores **automated clinical impression generation** from structured radiology report fields using state-of-the-art Large Language Models (LLMs). The project benchmarks multiple approaches including fine-tuning and prompt engineering on the **IU-XRay dataset**.

## üñºÔ∏è Graphical Abstract

![AutoImpress Graphical Abstract](images/autoimpress_graphical_abstract.png)

### üé™ Problem Statement
- **Input**: Structured radiology report fields (Findings, Indication, Comparison, etc.)
- **Output**: Concise, clinically accurate impression summaries
- **Challenge**: Bridging the gap between surface-level text similarity and clinical equivalence

### üèÜ Key Contributions
- Novel evaluation framework using GPT-4o clinical judgment
- In-depth analysis of model agreement and disagreement patterns
- Practical insights for medical NLP applications

---

## üë• Team

| Team Member | 
|-------------|
| **Yaniv Grosberg** |
| **Netanel Ohev Shalom** |
| **Aviel Shmuel** | 

---

## üì¶ Project Structure

```
AutoImpress/
‚îú‚îÄ‚îÄ üìÅ data_raw/                     # Raw datasets
‚îÇ   ‚îî‚îÄ‚îÄ indiana_reports.csv
‚îú‚îÄ‚îÄ üìÅ data_cleaned/                 # Processed datasets  
‚îÇ   ‚îî‚îÄ‚îÄ indiana_reports_cleaned.csv
‚îú‚îÄ‚îÄ üìÅ outputs/
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ flan-t5-base/            # FLAN-T5 results
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ generated_impressions_300_flan.csv
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ finetuned_model_test_results.csv
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ results_with_azure_gpt_judgment_baseline.csv
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ results_with_azure_gpt_judgment.csv
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ gpt-deepseek/            # GPT-4.1 & DeepSeek results
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gpt41_judged_results.csv
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ deepseek_judged_results.csv
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gpt4_1_acute_findings_vs_ground_truth.csv
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ deepseek_acute_findings_vs_ground_truth.csv
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ Analyze_Results_outputs/  # Analysis outputs
‚îÇ       ‚îî‚îÄ‚îÄ judgment_phrase_summary.csv
‚îú‚îÄ‚îÄ üìÅ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ üìì Preprocessing_EDA.ipynb   # Data exploration & cleaning
‚îÇ   ‚îú‚îÄ‚îÄ üìì flan-t5-base.ipynb       # FLAN-T5 training & evaluation
‚îÇ   ‚îú‚îÄ‚îÄ üìì gpt_deepseek.ipynb       # API-based model evaluation
‚îÇ   ‚îî‚îÄ‚îÄ üìì Analyze_Results.ipynb    # Comprehensive results analysis
‚îú‚îÄ‚îÄ üêç utils_file.py                # Shared utility functions
‚îú‚îÄ‚îÄ üìã requirements.txt             # Python dependencies
‚îî‚îÄ‚îÄ üìñ README.md                    # Project documentation
```

### üìä Output File Meanings
- **`uid`**: Unique identifier for each report
- **`generated_impression`**: Model-generated clinical summary
- **`true_impression`**: Expert-labeled ground truth
- **`gpt_equivalence`**: GPT-4o clinical judgment (Yes/No)


---

## üöÄ Quick Start

### 1Ô∏è‚É£ Clone Repository
```bash
git clone https://github.com/Yanivgg/AutoImpress.git
cd AutoImpress
```

### 2Ô∏è‚É£ Install Dependencies
```bash
pip install -r requirements.txt
```

### 3Ô∏è‚É£ Run Analysis Pipeline
```bash
# Data preprocessing and EDA
jupyter notebook notebooks/Preprocessing_EDA.ipynb

# FLAN-T5 model training and evaluation
jupyter notebook notebooks/flan-t5-base.ipynb

# GPT-4.1 and DeepSeek evaluation
jupyter notebook notebooks/gpt_deepseek.ipynb

# Comprehensive results analysis
jupyter notebook notebooks/Analyze_Results.ipynb
```

### 4Ô∏è‚É£ Explore Results
Generated outputs will be saved in the `outputs/` directory with detailed CSV files for further analysis.

---

## üè• Dataset

### IU-XRay (Indiana University Chest X-ray Reports)
- **Original**: 3,955 radiology reports
- **After Preprocessing**: 3,331 records (~13% reduction)
- **Fields**: Findings, Indication, Comparison, Image, MeSH, Problems ‚Üí **Impression**

### üìä Dataset Statistics
- **Text Length**: Findings ‚âà 190 chars, Impression ‚âà 76 chars
- **Vocabulary**: ~2,000 unique tokens
- **Common Pattern**: 

| Phrase                                      | Count |
|--------------------------------------------|-------|
| No acute cardiopulmonary abnormality        | 491   |
| No acute cardiopulmonary findings           | 189   |
| No acute cardiopulmonary abnormalities      | 168   |
| No acute cardiopulmonary disease            | 163   |
| No acute disease                            | 126   |
| No acute cardiopulmonary process            | 106   |
| No acute radiographic cardiopulmonary process | 93  |
| No acute cardiopulmonary abnormality identified | 80 |
| No acute pulmonary disease                  | 76    |
| No acute findings                           | 60    |


### üîß Preprocessing Steps
- Removed ~520 incomplete records
- Filled missing fields with "none provided"
- Replaced anonymized patterns with `[REDACTED]`
- Cleaned non-clinical tokens for prompt optimization

---

## üß† Methodology

### ü§ñ Models Evaluated

| Model                 | Type        | Configuration              | Platform             |
|-----------------------|------------|----------------------------|----------------------|
| **FLAN-T5 Base**      | Baseline   | Few-shot prompting         | Google Colab (GPU)   |
| **FLAN-T5 Fine-tuned**| Fine-tuned | 3 epochs, batch size 4     | Google Colab (GPU)   |
| **GPT-4.1**           | API-based  | Prompt engineering         | Azure API            |
| **DeepSeek-V3**       | API-based  | Prompt engineering         | Azure API            |


### üîÑ Processing Pipelines

#### FLAN-T5 Pipeline
```
Data ‚Üí Few-shot Baseline ‚Üí Fine-tune ‚Üí Validation ‚Üí GPT-4o Judge
```

#### GPT-4.1 & DeepSeek Pipeline
```
Data ‚Üí 50% Sample ‚Üí API Generation ‚Üí BERTScore ‚Üí GPT-4o Judge
```

### üìè Evaluation Metrics

#### Primary Metric: GPT-4o Clinical Judge
- **Approach**: Binary YES/NO clinical equivalence assessment
- **Prompt**: Expert medical comparison with strict clinical criteria
- **Focus**: Clinical interpretability over surface similarity

#### Secondary Metric: BERTScore F1
- **Purpose**: Semantic similarity measurement
- **Range**: 0-1 (higher = more similar)
- **Limitation**: May not capture clinical nuances

---
## üèÜ Key Results
###
![Clinical Equivalence](images/clinical_equivalence.png)
> Fine-tuning with FLAN-T5 significantly improves clinical equivalence from 1.3% to 47.7%, while large models like GPT-4.1 and DeepSeek exceed 77%.

###
![Key Performance Insights](images/key_performance_insights.png)
> Highlights include 36x improvement via fine-tuning, 129 flipped cases, and analysis of 3,331 radiology reports.

###
![Model Agreement](images/model_agreement_analysis.png)
> Strong agreement (83.7%) between GPT-4.1 and DeepSeek shows reliable model alignment for clinical judgment.

###
![Model Comparison](images/model_comparison_overview.png)
> A clear visual ranking of models based on clinical equivalence outcomes.

###
![Pattern Recognition](images/pattern_recognition_analysis.png)
> Visual breakdown of model performance on simple vs. complex patterns. FLAN-T5 benefits most from fine-tuning in both cases.

###
![Clinical Phrase Impact](images/clinical_phrase_impact_analysis.png)
> Different phrase types affect model agreement. "No acute" phrases show ~90% agreement, while complex and borderline cases reduce alignment.

###
![Fine-Tuning Impact](images/fine_tuning_impact.png)
> Fine-tuning changed 129 cases from ‚ÄúNo‚Äù to ‚ÄúYes‚Äù. This major improvement highlights the effectiveness of domain adaptation.

###
![Detailed Performance Metrics](images/detailed_performance_metrics.png)
> Breakdown of Clinical Equivalence, BERTScore F1, sample counts, and key takeaways for each model. Includes both semantic and clinical accuracy perspectives.

---

## üí° Key Insights

### üéØ Technical Insights
1. **Fine-tuning is Crucial**: Baseline FLAN-T5 performed poorly (1.3%) but fine-tuning achieved 47.7%
2. **Large Models Excel**: GPT-4.1 and DeepSeek reached ~78% without domain-specific training
3. **Prompt Engineering Matters**: Multiple refinement rounds were essential for optimal performance
4. **Clinical ‚â† Semantic**: High BERTScore doesn't guarantee clinical equivalence

### üè• Clinical Implications
1. **Pattern Templates**: Dataset's repetitive nature aids prompt steering but may limit generalization
2. **"No Acute" Advantage**: Models perform better on normal findings vs. complex pathology
3. **Human Validation Essential**: Even best models miss ~22% of clinical nuances
4. **Structured Input Benefits**: Organized report fields improve generation quality

### üî¨ Research Contributions
1. **Evaluation Framework**: Novel use of GPT-4o for clinical judgment evaluation
2. **Comprehensive Benchmarking**: Systematic comparison across model types and sizes
3. **Practical Insights**: Real-world applicability analysis for medical NLP systems
4. **Open Science**: Full reproducible pipeline with detailed analysis

---

## üî¨ References

### üìö Key Papers
- **Zhang et al. (2023)**: *Leveraging Summary Guidance on Medical Report Summarization* - Fine-tuning BART/T5 on medical reports, achieved superior performance on DISCHARGE and ECHO datasets
- **Ma et al. (2023)**: *From General to Specific: Domain Adaptation for Medical Report Generation* - HybridFusion approach combining general and medical LLMs, SOTA results on MIMIC-CXR
- **Van Veen et al. (2023)**: *RadAdapt: Lightweight Domain Adaptation of LLMs* - Efficient LoRA/Prefix tuning achieving clinical validation with minimal parameters

### üõ†Ô∏è Technical Stack
- **Models**: FLAN-T5, GPT-4.1, DeepSeek-V3
- **Evaluation**: BERTScore, GPT-4o Clinical Judge
- **Frameworks**: Transformers, PyTorch, Azure OpenAI API
- **Environment**: Google Colab Pro, VSCode, Jupyter Notebooks


## üôè Acknowledgments

- **Indiana University** for the IU-XRay dataset
- **Microsoft Azure** for providing free student accounts that enabled API access
- **Holon Institute Of Technology Academic Institution** for supporting this research project

---


**‚≠ê Star this repository if you find it useful! ‚≠ê**

*For questions, issues, or collaboration opportunities, please open an issue or reach out to the team.*

